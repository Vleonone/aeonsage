{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8736e88",
   "metadata": {
    "id": "ollama-integration"
   },
   "source": [
    "# ğŸš€ Ollama é›†æˆ (é›¶Tokenæˆæœ¬)\n",
    "\n",
    "åœ¨ Colab ç¯å¢ƒä¸­è®¾ç½® Ollamaï¼Œå®ç°é›¶Tokenæˆæœ¬çš„ AI æ¨ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b0041",
   "metadata": {
    "id": "install-ollama"
   },
   "outputs": [],
   "source": [
    "# å®‰è£… Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# å¯åŠ¨ Ollama æœåŠ¡\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# åœ¨åå°å¯åŠ¨ Ollama æœåŠ¡\n",
    "ollama_process = subprocess.Popen(['ollama', 'serve'])\n",
    "\n",
    "# ç­‰å¾…æœåŠ¡å¯åŠ¨\n",
    "print(\"â³ ç­‰å¾… Ollama æœåŠ¡å¯åŠ¨...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# éªŒè¯å®‰è£…\n",
    "!ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c87cb",
   "metadata": {
    "id": "download-models"
   },
   "outputs": [],
   "source": [
    "# ä¸‹è½½ Ollama æ¨¡å‹\n",
    "!ollama pull llama3.1:8b\n",
    "!ollama pull qwen2.5:7b\n",
    "!ollama pull mistral:7b\n",
    "\n",
    "print(\"âœ… Ollama æ¨¡å‹ä¸‹è½½å®Œæˆ\")\n",
    "print(\"ğŸ’¡ ç°åœ¨æ‚¨å¯ä»¥äº«å—é›¶Tokenæˆæœ¬çš„ AI æ¨ç†ï¼\")\n",
    "\n",
    "# æŸ¥çœ‹å¯ç”¨æ¨¡å‹\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84738756",
   "metadata": {
    "id": "configure-aeonsage"
   },
   "outputs": [],
   "source": [
    "# é…ç½® AeonSage ä½¿ç”¨ Ollama\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ç¡®ä¿ AeonSage é…ç½®ç›®å½•å­˜åœ¨\n",
    "os.makedirs(os.path.expanduser('~/.aeonsage'), exist_ok=True)\n",
    "\n",
    "# åˆ›å»ºé…ç½®æ–‡ä»¶\n",
    "config = {\n",
    "    \"models\": {\n",
    "        \"providers\": {\n",
    "            \"ollama\": {\n",
    "                \"apiKey\": \"ollama\",\n",
    "                \"baseUrl\": \"http://127.0.0.1:11434/v1\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"agents\": {\n",
    "        \"defaults\": {\n",
    "            \"model\": {\n",
    "                \"primary\": \"ollama/llama3.1:8b\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# å†™å…¥é…ç½®æ–‡ä»¶\n",
    "with open(os.path.expanduser('~/.aeonsage/config.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"âœ… AeonSage å·²é…ç½®ä½¿ç”¨ Ollama\")\n",
    "print(\"ğŸ’¡ æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨é›¶Tokenæˆæœ¬çš„ AI æ¨ç†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e932e",
   "metadata": {
    "id": "test-ollama"
   },
   "outputs": [],
   "source": [
    "# æµ‹è¯• Ollama é›†æˆ\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # æµ‹è¯• Ollama æœåŠ¡\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "    print(\"ğŸ“‹ å¯ç”¨æ¨¡å‹:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    # ä½¿ç”¨ AeonSage CLI æµ‹è¯•é…ç½®\n",
    "    result = subprocess.run(['pnpm', 'aeonsage', 'config', 'get', 'agents.defaults.model.primary'], \n",
    "                          capture_output=True, text=True, cwd='aeonsage')\n",
    "    print(f\"ğŸ¯ é»˜è®¤æ¨¡å‹: {result.stdout.strip()}\")\n",
    "    \n",
    "    print(\"âœ… Ollama é›†æˆæµ‹è¯•æˆåŠŸï¼\")\n",
    "    print(\"ğŸ’° æ‚¨ç°åœ¨äº«å—é›¶Tokenæˆæœ¬çš„ AI æ¨ç†æœåŠ¡\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æµ‹è¯•å¤±è´¥: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
